{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DBvyMOQ8tb6g"
   },
   "source": [
    "## Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SPb1pkSbNsQi",
    "outputId": "86d0bd3d-7377-4404-c5a9-7b500b95fc9b"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "from pytorch3d.transforms import quaternion_to_axis_angle, axis_angle_to_matrix, matrix_to_quaternion\n",
    "import os\n",
    "import PIL.Image\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "__DZ8Ktw_Hda"
   },
   "source": [
    "## Select dataset\n",
    "Datasets are in TUM format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "Tkvl0h5PbrLR"
   },
   "outputs": [],
   "source": [
    "datasets = {\n",
    "    'tum_desk': {\n",
    "        'directory': 'rgbd_dataset_freiburg3_long_office_household/',\n",
    "        'camera_factor': 5000.,\n",
    "        'camera_fx': 535.4,\n",
    "        'camera_fy': 539.2,\n",
    "        'camera_cx': 320.1,\n",
    "        'camera_cy': 247.6,\n",
    "        'camera_distortion': [0., 0., 0., 0., 0.], # k1, k2, p1, p2, k3\n",
    "        'camera_near': 0.3,\n",
    "        'camera_far': 8.,\n",
    "        'img_w': 640,\n",
    "        'img_h': 480,\n",
    "        'normalisation_factor': 10., # used to keep the model query region close to [-1, 1] range\n",
    "    },\n",
    "    'tiny_nerf': {\n",
    "        'camera_fx': 138.88887889922103,\n",
    "        'camera_fy': 138.88887889922103,\n",
    "        'camera_cx': 50.,\n",
    "        'camera_cy': 50.,\n",
    "        'camera_distortion': [0., 0., 0., 0., 0.], # k1, k2, p1, p2, k3\n",
    "        'camera_near': 2.,\n",
    "        'camera_far': 6.,\n",
    "        'img_w': 100,\n",
    "        'img_h': 100,\n",
    "        'normalisation_factor': 10., # used to keep the model query region close to [-1, 1] range\n",
    "    },\n",
    "}\n",
    "\n",
    "dataset_dict = datasets['tiny_nerf']\n",
    "device = torch.device(\"cuda:2\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinyNerfDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset_dict):\n",
    "        super(TinyNerfDataset).__init__()\n",
    "        \n",
    "        data = np.load('tiny_nerf_data.npz')\n",
    "        self.images = data['images']\n",
    "        self.poses = data['poses']\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        rgb = torch.tensor(self.images[idx]).permute((2,0,1))\n",
    "        pose = self.poses[idx]\n",
    "        axis_angle = quaternion_to_axis_angle(matrix_to_quaternion(torch.tensor(pose[:3,:3])))\n",
    "        translation = torch.tensor(pose[:3,-1])\n",
    "        gt_pose = torch.stack([translation, axis_angle], -1)\n",
    "        \n",
    "        return 0., torch.zeros((1, 100, 100)).to(device), rgb.to(device), gt_pose.to(device)\n",
    "    \n",
    "dataset = TinyNerfDataset(dataset_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SlamDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset_dict):\n",
    "        super(SlamDataset).__init__()\n",
    "        \n",
    "        self.entries = []\n",
    "        self.dataset_dict = dataset_dict\n",
    "        self.convert_tensor = torchvision.transforms.ToTensor()\n",
    "        \n",
    "        assoc_path = os.path.join(dataset_dict['directory'], \"associations.txt\")\n",
    "        gt_path = os.path.join(dataset_dict['directory'], \"groundtruth.txt\")\n",
    "        gt_timestamp = 0.\n",
    "        \n",
    "        with open(assoc_path) as assoc, open(gt_path) as gt:\n",
    "            while True:\n",
    "                line = assoc.readline()\n",
    "                \n",
    "                if line == \"\":\n",
    "                    break\n",
    "            \n",
    "                t, depth_path, _, rgb_path = line.rstrip().split(' ')\n",
    "                timestamp = float(t)\n",
    "\n",
    "                while gt_timestamp < timestamp:\n",
    "                    line = gt.readline()\n",
    "                    if line.startswith('#'):\n",
    "                        continue\n",
    "                    t, tx, ty, tz, qx, qy, qz, qw = line.rstrip().split(' ')\n",
    "                    gt_timestamp = float(t)\n",
    "\n",
    "                axis_angle = quaternion_to_axis_angle(torch.tensor([float(qx), float(qy), float(qz), float(qw)]))\n",
    "                translation = torch.tensor([float(tx), float(ty), float(tz)])\n",
    "                gt_pose = torch.stack([translation, axis_angle], -1)\n",
    "\n",
    "                self.entries.append((timestamp, depth_path, rgb_path, gt_pose))\n",
    "            \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.entries)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        timestamp, depth_path, rgb_path, gt_pose = self.entries[idx]\n",
    "        \n",
    "        depth = self.convert_tensor(PIL.Image.open(os.path.join(self.dataset_dict['directory'], depth_path)))\n",
    "        depth = depth.float() / self.dataset_dict['camera_factor']\n",
    "        depth[depth < self.dataset_dict['camera_near']] = 0.\n",
    "        depth[depth > self.dataset_dict['camera_far']] = -1.\n",
    "        rgb = self.convert_tensor(PIL.Image.open(os.path.join(self.dataset_dict['directory'], rgb_path)))\n",
    "        \n",
    "        return timestamp, depth.to(device), rgb.to(device), gt_pose.to(device)\n",
    "    \n",
    "dataset = SlamDataset(dataset_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FROM https://github.com/krrish94/nerf-pytorch/blob/a14357da6cada433d28bf11a45c7bcaace76c06e/nerf/nerf_helpers.py\n",
    "\n",
    "def cumprod_exclusive(tensor: torch.Tensor) -> torch.Tensor:\n",
    "    r\"\"\"Mimick functionality of tf.math.cumprod(..., exclusive=True), as it isn't available in PyTorch.\n",
    "    Args:\n",
    "    tensor (torch.Tensor): Tensor whose cumprod (cumulative product, see `torch.cumprod`) along dim=-1\n",
    "      is to be computed.\n",
    "    Returns:\n",
    "    cumprod (torch.Tensor): cumprod of Tensor along dim=-1, mimiciking the functionality of\n",
    "      tf.math.cumprod(..., exclusive=True) (see `tf.math.cumprod` for details).\n",
    "    \"\"\"\n",
    "    # TESTED\n",
    "    # Only works for the last dimension (dim=-1)\n",
    "    dim = -1\n",
    "    # Compute regular cumprod first (this is equivalent to `tf.math.cumprod(..., exclusive=False)`).\n",
    "    cumprod = torch.cumprod(tensor, dim)\n",
    "    # \"Roll\" the elements along dimension 'dim' by 1 element.\n",
    "    cumprod = torch.roll(cumprod, 1, dim)\n",
    "    # Replace the first element by \"1\" as this is what tf.cumprod(..., exclusive=True) does.\n",
    "    cumprod[..., 0] = 1.0\n",
    "\n",
    "    return cumprod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "i, j = torch.meshgrid(torch.arange(0, dataset_dict['img_w']), torch.arange(0, dataset_dict['img_h']), indexing='xy')\n",
    "ray_dirs_cannonical = torch.stack([(i-dataset_dict['img_w']*.5) / dataset_dict['camera_fx'],\n",
    "                    -(j-dataset_dict['img_h']*.5) / dataset_dict['camera_fy'],\n",
    "                    -torch.ones_like(i)], -1).to(device)\n",
    "rays_norms = torch.norm(ray_dirs_cannonical, dim=-1).to(device)\n",
    "\n",
    "def get_rays(camera_pose):\n",
    "    rotation_mat = axis_angle_to_matrix(camera_pose[..., 1])\n",
    "    ray_dirs = torch.sum(ray_dirs_cannonical[None, :, :, None, :] * rotation_mat[:, None, None, :, :], dim=-1)\n",
    "    ray_origins = camera_pose[:, None, None, :, 0].expand(ray_dirs.shape)\n",
    "\n",
    "    return ray_origins, ray_dirs\n",
    "\n",
    "def render_rays(model, ray_origins, ray_dirs, n_samples=48):\n",
    "    depth_values = torch.linspace(dataset_dict['camera_near'], dataset_dict['camera_far'], n_samples).to(ray_origins)\n",
    "    depth_values = depth_values.broadcast_to(list(ray_origins.shape[:-1]) + [n_samples])\n",
    "    \n",
    "#     # Randomize\n",
    "#     noise_shape = list(ray_origins.shape[:-1]) + [n_samples]\n",
    "#     depth_values = (\n",
    "#         depth_values\n",
    "#         + torch.rand(noise_shape).to(ray_origins)\n",
    "#         * (dataset_dict['camera_far'] - dataset_dict['camera_near'])\n",
    "#         / n_samples\n",
    "#     )\n",
    "    \n",
    "    query_points = (\n",
    "        ray_origins[..., None, :]\n",
    "        + ray_dirs[..., None, :] * depth_values[..., :, None]\n",
    "    )\n",
    "    \n",
    "    query_points = query_points.reshape((-1, dataset_dict['img_w'], n_samples, 3)).unsqueeze(0)\n",
    "    query_points = query_points / dataset_dict['normalisation_factor']\n",
    "\n",
    "    output = torch.nn.functional.grid_sample(model, query_points)\n",
    "    output = output.reshape((1, 4, -1, dataset_dict['img_h'], dataset_dict['img_w'], n_samples))\n",
    "    output = output.squeeze(0).swapaxes(0, 1)\n",
    "\n",
    "    sigma_a = torch.relu(output[:,:1,...])\n",
    "    rgb = torch.sigmoid(output[:,1:,...])\n",
    "    one_e_10 = torch.tensor([1e5]).to(device)\n",
    "    dists = torch.cat(\n",
    "        (\n",
    "            depth_values[..., 1:] - depth_values[..., :-1],\n",
    "            one_e_10.expand(depth_values[..., :1].shape),\n",
    "        ),\n",
    "        dim=-1,\n",
    "    )\n",
    "    dists = dists[:,None,...] * rays_norms[None,None,...,None]\n",
    "    alpha = 1.0 - torch.exp(-sigma_a * dists)\n",
    "    weights = alpha * cumprod_exclusive(1.0 - alpha + 1e-10)\n",
    "    rgb_map = (weights * rgb).sum(dim=-1)\n",
    "    depth_map = (weights * depth_values[:,None,...]).sum(dim=-1)\n",
    "\n",
    "    return rgb_map, depth_map\n",
    "\n",
    "def render(model, pose):\n",
    "    ray_origins, ray_dirs = get_rays(pose)\n",
    "    return render_rays(model, ray_origins, ray_dirs)\n",
    "\n",
    "def render_and_optim(model, optimizer, pose, rgb_gt, depth_gt):\n",
    "    optimizer.zero_grad()\n",
    "    rgb_pred, depth_pred = render(model, pose)\n",
    "    loss = torch.nn.functional.mse_loss(rgb_pred, rgb_gt)\n",
    "    \n",
    "#     depth_loss = torch.nn.functional.mse_loss(depth_pred, depth_gt)\n",
    "#     loss += 1e-1 * depth_loss\n",
    "\n",
    "#     # TV regularisation\n",
    "#     shifted = torch.roll(model, shifts=(1, 1, 1), dims=(2, 3, 4))\n",
    "#     tv_losses = torch.sqrt(torch.sum((model[...,1:-1,1:-1,1:-1] - shifted[...,1:-1,1:-1,1:-1]) ** 2., dim=1))\n",
    "#     loss += 1e-1 * torch.mean(tv_losses)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# for img in range(100):\n",
    "#     timestamp, depth_gt, rgb_gt, pose_gt = dataset[:500:20]\n",
    "#     rgb_gt = rgb_gt.unsqueeze(0).to(device)\n",
    "#     pose_gt = pose_gt.to(device)\n",
    "\n",
    "#     for iter in range(20):\n",
    "#         render_and_optim(voxel_model, model_optimizer, pose_gt, rgb_gt)\n",
    "\n",
    "# #     for iter in range(50):\n",
    "# #         render_and_optim(voxel_model, pose_optimizer, current_pose, rgb_gt)\n",
    "    \n",
    "#     if img % 10 == 0:\n",
    "#         ray_origins, ray_dirs = get_rays(pose_gt)\n",
    "#         rgb_pred, depth_pred = render_rays(voxel_model, ray_origins, ray_dirs)\n",
    "#         plt.imshow(rgb_pred.detach().cpu()[0].permute((1,2,0)))\n",
    "#         plt.show()\n",
    "#         plt.imshow(depth_pred.detach().cpu()[0,0])\n",
    "#         plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_set, val_set = torch.utils.data.random_split(dataset, [len(dataset)-1, 1])\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=4, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_set, batch_size=1, shuffle=False)\n",
    "voxel_model = torch.nn.parameter.Parameter(torch.rand((1,4,256,256,256)).to(device))\n",
    "model_optimizer = torch.optim.RMSprop([voxel_model], lr=1e-1)\n",
    "\n",
    "# _, _, _, pose_gt = dataset[0]\n",
    "# current_pose = torch.nn.parameter.Parameter(pose_gt.clone())\n",
    "# pose_optimizer = torch.optim.Adam([current_pose], lr=1e-3)\n",
    "\n",
    "def train_step(model, model_optimizer, data):\n",
    "    timestamp, depth_gt, rgb_gt, pose_gt = data\n",
    "    render_and_optim(model, model_optimizer, pose_gt, rgb_gt, depth_gt)\n",
    "    \n",
    "def val_step(model, val_loader):\n",
    "    loss = 0.\n",
    "    with torch.no_grad():\n",
    "        for data in val_loader:\n",
    "            timestamp, depth_gt, rgb_gt, pose_gt = data\n",
    "            rgb_pred, depth_pred = render(model, pose_gt)\n",
    "            loss += torch.nn.functional.mse_loss(rgb_pred, rgb_gt)\n",
    "    print('Validation loss: ', round(loss.detach().cpu().numpy() / 2., 5))\n",
    "    fig, axes = plt.subplots(1,2)\n",
    "    axes[0].imshow(rgb_pred.detach().cpu()[0].permute((1,2,0)))\n",
    "    axes[1].imshow(rgb_gt.cpu()[0].permute((1,2,0)))\n",
    "    plt.show()\n",
    "\n",
    "for epoch in range(100):\n",
    "    for i, data in enumerate(train_loader):\n",
    "        train_step(voxel_model, model_optimizer, data)\n",
    "        if i % 10 == 0:\n",
    "            val_step(voxel_model, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Dense SLAM with an Implicit Neural Representation.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
